# -*- coding: utf-8 -*-
"""A1_220150

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fp8R82T8XJc3anFbry6m8Ce8-aRv7QHx
"""

# Setting up the dependencies: (DO NOT EDIT THE LINES BELOW)
!pip install pymc==5.8.0 arviz==0.16.1 bambi==0.13.0 pymc-bart==0.5.2 kulprit==0.0.1 preliz==0.3.6 nutpie==0.9.1

# Import necessary libraries
import arviz as az  # For Bayesian data analysis and visualization
import matplotlib.pyplot as plt  # For plotting
import numpy as np  # For numerical operations
from scipy.special import binom, beta  # For binomial and beta distributions
import preliz as pz # defining priors
from cycler import cycler
import math

"""Q1. Suppose you have a jar with 4 jelly beans: 2 are strawberry-flavored, 1 is blueberry-flavored, and 1 is cinnamon-flavored. You draw one jelly bean at random from the jar.

    a. What is the sample space for this experiment?

    b. We define event A as the jelly bean drawn is strawberry-flavored and event B as The jelly bean drawn is not cinnamon-flavored. What are the probabilities of events A and B?

    c. Are events A and B mutually exclusive? Why or why not?

Consider strawberry flavoured bean as 0, blueberry flavoured bean as 1 and cinnamon flavoured bean as 2.

Then we defined the sample space of experiment as S and events A and B as tuple A and B

A. S=(0,0,1,2)

B. A=(0) and B=(0,0,2)

P(A)=2/4=0.5

P(B)=3/4=0.75

C. events A and B are not mutually exclusive because A and B have common occurances.

Q2. Previously, we defined a **Python function P** to compute the probability of an event using the naive definition of probability. Generalize that function to compute the probability of events when they are
 not all equally likely. Use this new function to compute the probability of events A and B from the previous exercise.

 Hint: you can pass a third argument with the probability of each event.
"""

def P_diff(A,Pi):
  unique_A=set(A)
  prob=0
  for x in unique_A:
    prob+=Pi[x]
  return prob

"""Let's define Pi tuple having probability of occuring i event at index i.
For this case let Pi=(2/4,1/4,1/4)
"""

Pi=(1/2,1/4,1/4)
A=(0,)
B=(0,0,2)
print("P(A) : ",P_diff(A,Pi))
print("P(B) : ",P_diff(B,Pi))

"""Q3.  Use PreliZ to explore different parameters for the Gaussian distributions. Use the methods `plot_pdf`, `plot_cdf`, and `plot_interactive`. We discussed the probability mass/density functions and the cumulative density function. But there are other ways to represent functions like the percentile point function ppf. Using the `plot_ppf` method of PreliZ, plot the percentile point function for the BetaBinomial and Gaussian distributions. Can you explain how the ppf is related to the cdf and pmf/pdf?"""

_, ax = plt.subplots(1, 2, figsize=(12, 5), sharex="col")
pz.BetaBinomial(alpha=10, beta=10, n=5).plot_ppf(ax=ax[0], legend="title")
pz.Normal(0, 1).plot_ppf(ax=ax[1], legend="title")

"""The PPF is the inverse of the CDF. While the CDF gives the probability that a random variable is less than or equal to a specified point, the PPF gives the point for which a certain probability is less than or equal to. Also we can say PDF/PMF are also related to PPF because ppf(x) is inverse of area of (pdf(X<x)).

Q4. Use PreliZ to compute the moments for the SkewNormal distribution for a different combination of parameters. Generate random samples of different sizes,
like 10, 100, and 1,000, and see if you can recover the values of the first two moments (mean and variance) from the samples. Report your obervations.
"""

_, axes = plt.subplots(4, 1, figsize=(10, 20), sharey=True, sharex=True)
axes=np.ravel(axes)

para=((0,1,3),(-1,3,2),(-4,5,6),(2,1,5)) #considering different values of parameters

for ax,(a,b,c) in zip(axes,para):

  print("For parameter : ",(a,b,c))

  Skew_Normal_dist=pz.SkewNormal(a,b,c)
  Skew_Normal_dist.plot_pdf(ax=ax,moments=["m", "d", "s", "k"])

  sizes=(10,100,1000)

  for size in sizes:
    sample=Skew_Normal_dist.rvs(size) # getting sample of given size
    mean=np.mean(sample) # calculating mean of sample
    std=np.std(sample) # calculating standard deviation of sample
    print("size : ",size,", mean : ",mean,", standard deviation : ",std)

  print('\n')

"""We were able to get quite close to the original values of mean and standard deviation as we increased size of samples for all the values of parameters."""